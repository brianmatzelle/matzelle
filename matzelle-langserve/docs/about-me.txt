---RESUME---
Brian Matzelle
Garden City, NY | (516) 413-2823 | bmatzel1@binghamton.edu | LinkedIn | Porfolio | GitHub
EDUCATION
Binghamton University, SUNY | Thomas J. Watson College of Engineering and Applied Science	
Bachelor of Science in Computer Science	Expected May 2024
Relevant Coursework: Data Structures & Algorithms, Design Patterns, Design & Analysis of Algorithms, Intro to Machine Learning, Programming Languages, Java OOP, Advanced Computer Architecture
TECHNICAL SKILLS
Languages: Python, Javascript,  C++, HTML, CSS, SQL, Rust, C,  Java, Go
Frameworks & Software: AWS, Temporal, Google Cloud,  Node, React, NLP, spaCy, Microsoft SQL Server, React Native, Next.js, Expo, React Navigation, Git, Windows, MacOS, Linux, XGBoost, sklearn, LangChain, FAISS
PROFESSIONAL EXPERIENCE
Software Engineer Intern, Med-Metrix | Garden City, NY	June 2023 - December 2023
Eliminated overnight model training after refactoring XGBoost classification model, increasing performance by 200%
Preprocessed proprietary language data to finetune Falcon-7b large language model (LLM) to write insurance appeals
Improved classification model accuracy from 80% to 90% by refining data preprocessing methods to heighten efficacy of written appeals and optimizing revenue potential
Built data extraction pipelines with Airbyte’s CDK to ingest FHIR resources into S3 buckets via custom connectors, helping with database cloud migration
Designed an LLM workflow with LangChain for accurate patient medical record retrieval, creating dedicated FAISS vector stores for each patient, and generating appeal documents
Collaborated closely with legal experts to ensure AWS development environments remained HIPAA compliant when projects included Protected Health Information (PHI)
Co-Founder & Software Engineer,  OnePolicy, LLC | Vestal, NY	November 2022 - May 2023
Spearheaded Next.js website prototypes to validate complex technical solutions before full-scale implementation
Demonstrated adept project management abilities through the effective use of agile methodologies, including task prioritization and meticulous project timeline monitoring, ensuring on-time delivery of high-quality website updates
Cultivated strong project management skills by implementing agile methodologies, effectively prioritizing tasks, and closely Translated business requirements into precise React Native technical specifications, seamlessly integrating web-app functionalities and managing third-party API interactions
Improved project execution by devising strategies, enhancing internal communication, and aligning company expectations
Software Engineer Intern, TJ Russo Consultants | Islandia, NY	June 2022 - August 2022
Collaborated with a team of 4 developers to build an NGINX based live-streaming service to improve client outreach
Utilized Expo library to develop client live-streaming mobile app prototype, bettering the assessment of potential solutions
Developed email templates that contributed to a 10% increase in client engagement.
Optimized the local infrastructure and conducted hardware upgrades, resulting in reduced technical issues and outages, safeguarding time-sensitive revenue streams
KEY PROJECTS
LendaHand, HackBU 2023 Submission	 February 2023
Won Best Civic Engagement Hack, sponsored by J.P. Morgan, for exceptional community impact
Awarded Best Geo Hack, sponsored by CAE, for innovative geospatial application
Outperformed 120+ participants as team lead in 24-hour hackathon by developing cross-platform community outreach app
Called the Google Maps API to display user event locations, enhancing user experience and event planning capabilities
Reddit & Twitch Toxicity Data Analysis	August 2023 - Present
Teamed with 2 classmates to author analytical plan, comparing toxicity on r/LivestreamFail with corresponding Twitch clips
Developed Dockerized data pipeline to scrape new data, judge it with external API, then store it in MongoDB database
Implemented Temporal.io to handle workflow orchestration and scheduling, optimizing retry policies to prevent data loss
Determined communities with the Girvan Newman algorithm and visualized findings with NetworkX knowledge graph
Communicated all proposals and implementations effectively via LaTeX writings, surpassing project criteria
NFL Play Predictor	July 2023 - Present
Cleaned and Preprocessed large tabular datasets using pandas, categorizing data and optimizing it for machine learning
Achieved 66% accuracy of drive outcome based on real play by play statistics with XGBoost classification model
Analyzed the model with sci-kit learn (sklearn), allowing for substantial accuracy improvements after each code refactor
Chat.tv, a Live Streaming AI Chat Service	April 2023 - Present
Multi-threaded app to maintain smooth GUI performance while receiving API responses in real-time
Engineered a simulated live streaming platform enabling small streamers to interact with large language model bots
Integrated SpeechRecognition to transcribe user speech into text for seamless communication with OpenAI API bots
Scaled platform to support 10,000+ bots simultaneously in "livestreaming chat" environment
---END RESUME---

--MISC. HOBBIES--
Brian Matzelle is a senior at Binghamton University studying Computer Science.

I enjoy programming, producing music, skateboarding, and playing video games.
I began producing music at 16. 
I've been producing since then, continuosly learning and improving at the art. This is how I realized the value in sticking with a passion for a long time. 
I learned that skills take time to develop and patience is important, which helps me every time I'm learning a new framework in development. 
Analyzing my passion for creating music is also what led me to chosing computer science as my major. I realized that I like working on things that I'll be able to maintain afterwards. It's satisfying being able to listen to a song I've made, or use a product I've built.

I view skateboarding very similarly to my other hobbies. 
Once you're good at it, there's a level of respect that you demand from everyone else who'd good, because they understand what it took to get to where you are now. 
This feels similar to software engineering and Super Smash Bros. Melee (my favorite video game). 
If something is difficult, odds are it's worth chasing (not everything of course, but in regards to hobbies).
As far as tricks go, I know how to ollie, pop shuvit, heelflip, kickflip, and I'm currently working on treflips and inward heelflips.

My favorite video game is Super Smash Bros. Melee. I've probably spent 1000+ hours on it (mostly in highschool).
I'm a falco main and I just placed in the top 8 at my school's biweekly tournament! Feelsgoodman.
--MISC. HOBBIES--

---PROGRAMMING/PROJECT DESCRIPTIONS---
I started programming in high school and decided to pursue it once realizing how creative it could be. 
University provided me with strong fundamentals and a passion for learning new technologies, which I continues to do in his free time.
I much prefers to learn by doing, and has worked on a variety of projects, including a live streaming chat service, a reddit and twitch toxicity data analysis, and a play predictor for the NFL.
Being a software engineer has allowed me to combine his love for technology and creativity, and I'm excited to continue to do so in the future.
I developed the live streaming AI chat service over the course of few months for fun. The program allows someone to simulate a Twitch.tv chat room with AI bots, powered by OpenAI. Notably, I learned how to multithread confidently, handle API calls, and build a smooth, responsive UI.
Alongside two classmates, I developed the reddit and twitch toxicity data analysis project over the course of a semester for CS415 Social Media Pipelines, a class at Binghamton University. The Dockerized program runs on a schedule with Temporal, scrapes data from r/LivestreamFail and each clip's corresponding Twitch chat, and analyzes the toxicity of each comment with the ModerateHatespeech API. It then stores the data in a MongoDB database, to later be analyzed by the Mistral Zephyr large language model. The analyzed data is then grouped with the Girvan Newman algorithm, and visualized with NetworkX. I learned how to use Temporal, Dockerize a data pipeline, and use NetworkX to visualize data.
After being introduced to machine learning at his internship at Med-Metrix, I decided to pursue it further. I developed the NFL play predictor over the course of a few weeks for fun, but to also help my team at work. The program uses XGBoost to predict the outcome of a drive based on real play by play statistics. Here, I learned how to use XGBoost, and how to clean and preprocess large tabular datasets.
---END PROGRAMMING/PROJECT DESCRIPTIONS---


---INTERVIEW QUESTIONS---
Approach with Conflict:
In conflict, it's best to be truthseeking. This might manifest differently depending on the situation, whether it be raising the issue with a higher up, or doing research to find the right answer.

Potential Tradeoffs in a Project:
Tradeoffs I've had to make in the past include sacrificing performance for readability, or sacrificing readability for performance. It's important to find the right balance between the two. On a higher level, we've had to sacrifice 

Difficult Problem You've Solved:
One problem I'm continuing to work at is properly predicting the amount of time a project will take. 
I'm ambitious, so I have a tendency to underestimate how much time a project will take, but I've been taking that account recently to alter my estimates.
The best way to address this in the short term has been with effective communcation. Being communicative about the project's timeline as it changes is of utmost importance.

Optimal Time Management:
Emails in the morning, meetings in the afternoon, and coding in the evening. I find that I'm most productive at night, so I try to schedule my day around that. 
Also, I definitely prefer to have an uninterupted chunk to dedicate to coding.

Dealing with Close Deadlines:
Communication!

Thoughts on Improving Software Performance:
In my opinion, staying curious about the engineering tasks at hand is the best way to improve software performance.
Treating the task at hand as a puzzle and experimenting with different solutions, then testing and comparing each solution has been the best approach for me.

Thoughts on Communication:
Communication is key! It's better to communicate problems early instead of letting them linger and combust later on.

What are you most proud of:
I'm definitely super proud my submission for HackBU2023. 
My team of 3 created a mobile application with React Native + Expo, where community members could post events asking for helps with tasks they needed to get done.
Since the hackathon was only 24hrs long the app was unfinished, but I'm really happy with what we were able to get done.
The submission is posted here: https://devpost.com/software/lendahand-oq1snb
---END INTERVIEW QUESTIONS---

--MISC. QUESTIONS--
What's your favorite food:
pizza and candy

What's your favorite color:
yellow

How was this built:
This was built with LangServe, the server library for LangChain. I use OpenAI for the LLM responses, and a Faiss vector store so the LLM can recall answers to questions such as this xD. 
LangServe made it easy to build this RAG pipeline.

Are you working on any projects currently:
Currently, I'm working on an early stage startup with a few friends of mine, which implements RAG pipelines such as this. Feel free to email me and ask me more about it at brian@matzelle.co !

--END MISC. QUESTIONS--


hi:
hi i'm brian, ask me about my work experience, projects, or academic work!
hi


---BEGINNING OF ETHICAL ACADEMIC PAPER---
Brian Matzelle
Writing 111
17 May 2021
Annotated Bibliography

What effects can internet addiction have on an individual, and where do these effects stem from?

Recent innovations such as social media, the smartphone, and the internet have proven to be more harmful than many realize, affecting the wellbeing of its users on a very personal level. This means wounding the mental health of many for the sake of profit, using unethical business practices because the capitalistic society it has been brought up in is allowing it. Specifically, happiness and the feeling of identity are greatly impacted. The solution is to help oneself by being aware of the algorithmic practices, as it is likely too far gone to change the current structure of the internet. This way, users can be more conscious about their decisions and spend time willingly, bringing back a sense of identity and purpose. Mental health would then benefit from the newfound freedom from addiction.

Toronto, E. (2009). Time out of mind: Dissociation in the virtual world. Psychoanalytic Psychology, 26(2), 117–133. https://doi.org/10.1037/a0015485

	In “Time out of mind: Dissociation in the virtual world”, Toronto discusses internet addiction, how it may form, and the effects it can have on an individual. Toronto specifically mentions two individuals that she has studied and interviewed, aiming to provide an objective perspective on the lives of those who spend much of their time on the internet. When prefacing the essay with general effects the internet can have on an individual, the author notes that our culture fully embraces the alternate reality that is painted for us online. This then allows us to become obsessed with seemingly useless activities in the form of  “online journals, chat rooms, and excessive involvement with video games, as well as Internet pornography and sexual solicitation” (Toronto 1). The root of Toronto’s worry starts here, as the paper then moves on to study the effects these common internet activities have on several subjects. The author’s worry is supported by conclusions drawn from the subjects studied.
	What makes this essay powerful is how unique it is. Rather than gathering data and drawing conclusions, it provides an inside look and analysis into the lives of those who are impacted most by the internet. Toronto is very clear in her claim that society should be cautious of overindulging in online content, and gives analysis-based reasons why. The author tells a true story of a boy named Timmy, one that may be relatable to those who have grown up during the internet era. The paper succeeds in its analysis, as although it is anecdotal, the perspective is more personal than anything data can show. Timmy is evaluated and documented through Toronto’s eyes with the intent to observe, subsequently proving her claim. Toronto’s argument and research is helpful, as it would show worrying effects too much internet time can have on an individual level. The argumentative research paper looks to discuss the negative effects of internet addiction, as well as the powers that cause them. This article provides an opinion on that topic from a Clinical Psychologist who has a Ph.D., making the analysis worthy of discussion. 
	The habits and traits of “Timmy” and his studied peers allow for a unique perspective that is personal, rather than just listing data. This analysis could very likely be used to invoke pathos from the reader, explaining the dangers of technology addiction in a new light. Toronto’s essay would help the reader care about the issue at hand, giving a convincing reason as to why changes need to be made in regards to techniques used to keep users addicted. In addition, the story-like descriptions of her subjects may even allow the reader to relate to those studied, making the issue seem more apparent than realized before. Utilizing Toronto’s analysis is key when aiming to spread awareness of the dangers of internet addiction.

Ghaemi, S. N. “Digital Depression: a New Disease of the Millennium?” Wiley Online Library, 18 Jan. 2020, https://doi.org/10.1111/acps.13151

“Digital Depression: a New Disease of the Millennium” by Ghaemi points out the worrying increase of depression and suicide amongst adolescents, and its connection to digital technology created since the invention of the smartphone. This includes social media as well, being one of the main points mentioned in the article. Unlike the first source’s anecdotal approach, Ghaemi attempts to prove his claim by using statistics that are difficult to deny. The goal of his paper is to prove that screen time and social media do in fact catalyze depression for those who are already susceptible to the illness. Only then will it be possible to treat the growing numbers of depression amongst teenagers and young adults. Ghami intentionally sets data from several sources together at once to prove that there is more than a correlation with screen time and depression. In the article it is noted that reportedly 45% of American teenagers are online constantly, a number that has maintained a direct relationship with the growing reports of depressive symptoms, which is currently said to be 22% of adolescents (Ghaemi 2). This is only one example of the many times studies’ findings were connected, and parallels were drawn. 
The data chosen is collected from various surveys and clinical studies, essentially making this paper a compilation of relevant data to the issue at hand. The data successfully connects the research with analysis by repeatedly drawing parallels between new and unique data that stay relevant to the topic. Furthermore, Dr. Ghaemi is certified with an MD in Psychiatry from the Tufts University School of Medicine. 
Ghaemi’s intention here is to recognize the problem, so steps can be taken to move forward with treatment. This intention is incredibly relevant in this research process, as it can help emphasize that awareness must be spread and the issue must first be recognized so it can later be addressed. It falls in line with the purpose of the argumentative research essay. Again, this article can help shed light on the effects of the common overconsumption of internet media. That way, less time will be spent explaining the effects, so more time can be devoted to analyzing the root causes of the problems.

Pavlov, I. P. (1927). Conditioned Reflexes: An Investigation of the Physiological Activity of the Cerebral Cortex. Translated and Edited by G. V. Anrep. London: Oxford University Press. p. 142.

	In this study, Russian physiologist’s objective was to study the “physiological activities of the cerebral hemispheres.” In other words, Pavlov was interested in studying the relationship between two stimuli and a given subject to see if new learned responses could be produced. Pavlov’s example utilized a bell and food as the stimuli, with the dog as the subject. Every time the dog was fed, the same bell would ring. Pavlov found that after enough feedings with both stimuli, the dog would begin to salivate at the bell's sound, regardless of whether there was food or not. The experiments used dogs as the subject, however it has since been understood that this study is just as applicable to human beings. Here Pavlov proved that both people and animals could be conditioned to learn new habits, even if they are unaware of them.
Conceptually this is known as a Classical Conditioning, one of the most influential psychological findings of humanity's existence. Pavlov is a renowned figure in the world of psychology, with his findings being taught in every class studying the subject. The study has proven its value and credibility throughout time, as the concept has taken hold over much of modern consumerism. Included in modern consumerism is the smartphone, as well as digital technology. Classical conditioning is one of the most important pieces of the argument here, as it helps explain what is causing the increasing numbers of depression, rather than the effects, which the other sources covered.
Pavlov’s study is critical here as it helps address the root of the problem, rather than observing the effects. Once understanding this study, conclusions can be drawn that explain the increased time spent on technology, and the habits formed that are not even questioned by the user. This study will also help with the analysis of notifications and the loud high pitch sound that comes along with them. Habits formed through classical conditioning lead to a life where one does not feel in control, because their daily routines are dictated by digital technology. This is how a connection between sources is made.

Oulasvirta, A., Rattenbury, T., Ma, L. & Raita, E. (2012). Habits make smartphone use more pervasive. Personal and Ubiquitous Computing, 16(1), 105–114. 10.1007/s00779-011-0412-2

	“Habits make smartphone use more pervasive” is a research study that looks further into habits formed by smartphones, hoping to explain how classical conditioning has affected society through modern technology. Checking behaviors are known to exist, it is common knowledge at this point. Here, data is collected and presented so it is ready to be analyzed by those looking to draw conclusions from it. That being said, the researchers did provide conclusions of their own, stating that “although repetitive habitual use is frequent, it is experienced more as an annoyance than an addiction” (Oulasvirta). The thought is that users engage with notifications for a small reward, as it is an easy task to complete. It is claimed that this habit-formation is used as  “an opportunity for making smartphones more “personal” and “pervasive” by companies leading the digital market (Oulasvirta).
	The data presented in this study is very straightforward, essentially proving the addiction that many people have to their smartphones. Here the study does its job, it provides data collected from surveys that were later graphically analyzed, creating clear numerical conclusions that are easily digestible for the reader. To further prove the study’s credibility, Oulasvirta is a respected scholar, with many studies that have been conducted and cited by other researchers.
	Habits are an important part of the discussion, as this study helps reinforce the observation of classical conditioning in modern technology. Again, in research this article aims to support the argument by locating the root of the issue, rather than observing the effects. The parallel that must be drawn is that of classical conditioning and increased screen time, which then will lead to depressive symptoms in adolescents. Habits and addictions are formed without the consent of the user, which will be a major point in the argumentative research essay.

Hema Yoganarasimhan (2020) Search Personalization Using Machine Learning. Management Science 66(3):1045-1070. https://doi.org/10.1287/mnsc.2018.3255

In the research article “Search Personalization Using Machine Learning” Yoganarasimhan provides a framework for the concept of machine learning from user data. This article aims to inform the reader on the process of machine learning, noting the different types of algorithms used. Separate from usual research, this article actually aims to pitch a product to the reader, a product that is a mathematical algorithm that predicts users’ next clicks. Pursued customers of this research team are big tech companies like Amazon and Google. The purpose of the article is clear, which is to explain how exactly the algorithm manipulates internet users. Its goal is to keep users clicking for as long as possible, so the company utilizing the algorithm makes the most money possible, typically from advertising revenue.
	Presenting the mathematical equations is Dr. Yoganarasimhan a marketing expert who received her Ph.D., M.A., and M.Phil. in Marketing and Business from Yale School of Management. Yoganarasimhan is a leading expert in the field of quantitative marketing, making this paper more than relevant at this time, in the digital environment. Similar to the last two papers, this will help lead to the cause of the problem, rather than observing the effects.
	The article at hand is incredibly important for the research papers analysis section, as it will provide insight into the big tech companies’ point of view, so their intentions can be understood. Algorithms becoming more and more effective will further improve the argument for addiction, and how it is intentionally created by big companies. This will be what allows the final conclusion to be drawn, which is that ultimately, the business practices of the leading companies in digital technology are unethical, and awareness must be spread.
	

Brian Matzelle
Writing 111
16 April 2021
Argumentative Research Paper
The Natural Progression of Technology in a Free Society
	The Natural Progression of Technology in a Free Society. A confusing name for a topic that almost everyone in modern society has been affected by, aware of it or not. Technology surrounds us everywhere we go. It travels with us in our pockets, hoping to never leave our side. Modern technology, that is innovation that is less than two decades old, has completely shifted the way human beings live their lives daily, especially affecting social life. The creation of social media, the smartphone, and the full adoption of the internet has increased interpersonal communication by immeasurable amounts, to say the least. This drastic change, however, has come with concerning consequences that have not yet been fully realized, as there is a lack of scrutiny on such a new field. Mental health statistics are becoming more alarming, with much of the fault failing to fall on modern technology, where the problems stem from. Adolescents are growing up in a time unfamiliar to their parents and facing the consequences for the actions of others. This may be confusing at first, but it will make sense once the manipulative business practices of these large tech companies are understood. There is much going on behind the screen that is unethical, yet has not been questioned because of how new the field is, and how much money there is to be made. Modern time is comparable to the start of the Industrial Revolution, as we are currently witnessing the beginnings of the Information Age. Similar to the Industrial Revolution, many business practices need to be discussed, practices that the consumers should at least be aware of, providing them with fair judgment when partaking in online activities. Recent innovations such as social media, the smartphone, and the internet have proven to be more harmful than many realize, affecting the wellbeing of its users on a very personal level. This means wounding the mental health of many for the sake of profit, using unethical business practices because the capitalistic society it has been brought up in is allowing it. Specifically, happiness and the feeling of identity are greatly impacted. The solution is to help oneself by being aware of the algorithmic practices, as it is likely too far gone to change the current structure of the internet. This way, users can be more conscious about their decisions and spend time willingly, bringing back a sense of identity and purpose. Mental health would then benefit from the newfound freedom from addiction.
	To start it would be wisest to understand what the problem on hand is, then spend the majority of the paper explaining how it came to be, and finally how to move forward. To be clear and precise: the youth is becoming more depressed year by year, so the goal here is to understand and explain why. Research has shown that “depressive symptoms, suicide-related outcomes, and suicide deaths among adolescents all rose during the 2010s. These increases follow a period when mental health issues were declining or stable (see Table 1). Between 2009/2010 and 2015, 33% more adolescents exhibited high levels of depressive symptoms” (Twenge). This finding is concerning, to say the least, as adolescents are next in line to be running society. Depression will only hurt the generations following, especially if this is only the start of a trend that we cannot yet foresee. 
	The correlation between relevant technology use and depressive symptoms is not limited to adolescents though. Studies have shown that adults who spend more time watching TV also develop depressive symptoms at higher rates than normal. According to a study done on a large population in China, “ results indicate that time spent in TV watching was significantly associated with increased depressive symptoms, which are in accordance with previous work showing a link between TV watching and mental health” (Yu, Bin, et al). This research is important, as it clearly shows the correlation between screen time and depression. There are some differences that cause the internet to be more worrying, those of which will be addressed later. Again, this is to show that TV acted as a precursor to what we are witnessing now, with its effects on mental health and happiness specifically.
	A tactic that has been understood continuously and used by all of the big tech companies is instant gratification. This may sound familiar, as recently some of the worries behind the practice have been garnering attention. The dangers of instant gratification have been spoken about before in “Time Out of Mind: Dissociation in the Virtual World.” Here, Dr. Toronto notes the difference between the online world and real life, a difference that is often forgotten by many of the internet’s users. Toronto describes the internet as fantasy, explaining “when the world of fantasy in any form becomes a seductive alternative that breaks with ongoing experience, it disrupts the biographical narrative that is critical to the development of agency and the functioning of the relational self” (Toronto 5). The relevance here is: instant gratification is what causes the feeling of fantasy online. This idea will be referenced and built upon later, but now it is important to remember the connection between instant gratification and fantasy.
	Algorithms have become more effective, tracking down users and predicting their wants and desires with more accuracy. This has gotten to the point where it is as if there is a mirror image of the user, acting to give the user what it wants, keeping them entranced in whatever they are currently doing. A study done by Jin Kang and Bingjie Liu in 2019 demonstrates the manipulative ability algorithms have by implementing social comparison in media shown. Conducted experiments found it is fairly easy to influence a user’s likings, with this experiment using similarity tactics to prove the point. “Second, we found a positive indirect effect of overall similarity (both high and moderate cues) on participants’ liking for a comparison target via perceived similarity” (Kang, Jin, and Liu Bingjie 11). To help understand this a little more, the researchers are essentially concluding that it is fairly easy to predict a user’s actions if the manipulator knows what the subject is similar to, or how they identify themselves. They then can make comparisons with what is on screen and stay engaged with the media. This can then be used for more practical purposes, placing users into groups with those who think similarly to them, so there is constant comparison and reassurance amongst those partaking in the conversation. This creates a thought bubble where ideas are radicalized and individuality is taken away, then dividing those who are in different groups further. This is unhealthy as stated in the thesis, because it takes away one’s personal identity, and only makes them the group that they are a part of. Moreover, it is dangerous, as it radicalizes those who are in the group, making them more likely to go to the extreme publicly, when arguing a point.
	What has yet to be explored and discussed is why this increase in depression is occurring, especially in adolescents nowadays. Sure, it is screen time that depressive symptoms are correlated with, but why? What about screen time causes this to happen, and what exactly is being consumed? This is the part of the conversation that is fully explored here, hoping to dig deeper into the problems and push the narrative of good ethics in technology further.
	One may argue that the claim against algorithms and internet use is unfair though, the internet has changed civilization forever, as we have become more intelligent and communicate faster than ever. It may seem foolish to argue against such a monumental movement, especially while using it in almost every aspect of our lives, ironically while writing this paper as well. Specifically, a skeptical reader might make a fair point here, that mass manipulation has always existed in human life, especially in modern civilization. This would imply that this is nothing new, merely a product of human discourse. The reader may conclude, those who are at the highest positions of power will do whatever is necessary to retain and benefit from it, so what is different now? To take on this argument, it would make the most sense to understand how the masses have been manipulated throughout history. Then it may be more apparent as to why the algorithms used on the internet are unique and more dangerous than past methods of manipulation. Throughout history, those taking part in exploiting the masses were most commonly political leaders and people of power. Tactics used in past civilizations were typically propaganda and fear, with the goal being to ensure the people’s trust for the ruler,  therefore guaranteeing his power over the people. To a citizen, this was something that was dealt with every once in a while. Say a Roman goes to the town hall and hears about how great Caesar is, or an American citizen in the fifties watches an advertisement for a new product that can be purchased. These are all forms of manipulation, although less daunting. It is clear that these messages have intent behind them, but at the end of the day, they are only that—messages that the viewer actively decided to pay attention to. This was done with the viewer’s permission, therefore making it ethical. What we are witnessing now is different. Modern technology (the smartphone, video games, social media, etc.) abuses the subconscious, not allowing its user a chance to step back and decide if they would like to take part in actions that would benefit the speaking party. In modern times the speaking party is not the advertiser, but the social media site asking you not to buy their product, but only to open the app and scroll through for a few minutes. The speaker’s intent was always to convince the listener to do something of the speaker’s wish, but now the speaker does not have to ask for permission because of tactics discovered that affect our dopamine levels, like instant gratification and fast progression. Dopamine is a natural chemical created in our brain that acts as the reward center, typically releasing when we eat something we are craving, or after sex. The exploitation of dopamine to keep people addicted to an online program was spoken about in a 2013 study, stating “offer enough rewards to entice the user to return. Gremes give the pleasure of a dopamine surge, a similar pathway to that of drug and other addictions. By using a dopamine reward, researchers manipulate the user into continuing to play” (Graber 2). This is the primary difference between manipulation of the past, and manipulation now. Back then, consent to listen was needed, but now, as long as a dopamine addiction is initiated, addiction will be hard to break and consent will no longer be possible. It will get harder and harder to stop, and every day a user continues to surf his favorite website, the more addicted he will get. This strips the user’s freedom as it becomes an addiction that they are unaware of, and did not consent to. This can then cause a feeling of being trapped, without understanding the reason why.
	From here on it will start to make more sense. Everything that is placed in front of the masses is done intentionally because if everything wasn’t there would be missed opportunities to make the most profit. Profit is what drives big tech, it is the reason why technology has taken a turn for the worse, looking to keep our eyes attached to the screens for as long as possible. The longer we are staring at the screens, the more money they make by selling data to advertisers. Therefore, they will do whatever is legally possible to acquire every last bit of information from their users. This means using everything, yes everything they have at their disposal. Almost every sensory ability we have is exploited by our smartphones to condition us into being addicted to our phones, as mentioned in the previous paragraph. This equates to people making many reward-based checks that have little to no purpose (Oulasvirta, Rattenbury, Ma, & Raita, 2012). It is even likely that you, the reader, have had a notification pop-up on your screen during this reading (CleverTap, 2018). There is no such thing as something free, there must always be a give and a take. The users provide the data as payment, and big tech companies act as the dealer, giving the users their timely dose of dopamine, whenever they crave it. It is a cruel interaction when put into these terms, but that is exactly what it is. Think about a phone notification. It vibrates, makes a loud high pitched sound, and lights up the screen, most likely with the color red somewhere. This pertains to classical conditioning, where the subject starts to associate given stimuli with a result (Pavlov, 1927). Psychological tactics like these attack the subconscious making them inherently unethical. As stated before, it allows online companies to bypass the need for permission of the user, as the user becomes addicted to the service.
	From these practices, mental health has suffered. Now it may be easier to understand why, as many of the consumers—especially adolescents, are addicted to everything they consume online. Furthermore, it is incredibly easy to get whatever the addict wants, whenever they want it. This was spoken about earlier by Dr. Toronto, mentioning that the internet is a fantasy world, and it has found a way to intertwine itself with real life. Everything is amazing online, almost too good to be true. There is no longer any difficulty in getting what is wanted, it is not like we have been biologically programmed to exist. Entertainment is only one click away. Now rather than putting in the effort, researchers do it for the user, spending lots of time and money to find the best algorithms that will predict what is wanted, all to keep the user’s eyes locked on the screen. In a pitch made that was aimed towards Amazon and Google, this researcher stated “The goal of reranking is to promote URLs that are more likely to be relevant to the user and demote those that are less likely to be relevant. Relevance is data-driven and defined based on clicks and dwell times” (Yoganarasimhan 1056). This is an example of what the problem is, straight from the source of it. This is a researcher looking to sell his algorithm to big tech, blatantly stating what the goal is. The only wish is to keep the users engaged, as there is no thought about the user’s wellbeing, only profit. It is now starting to make sense why 45% of American teenagers claim they are “online constantly” (Ghaemi 2). In conjunction with the statistic showing that 22% of teenagers are depressed, the thesis claim makes more sense (Ghaemi 2). Adolescents are addicted to pointless websites, games, and applications which subsequently will lead to depression. It is a chain of events that starts here. Too much leisure time leads to a lack of purpose, which is often felt even if not realized. A lack of direction in adolescents is apparent because there are no hobbies present besides wasting time on social media or video games, even maybe both. If there is no time dedicated to pursuing a passion and developing interests, why would somebody know what to do with their life? This uncertainty then leads to a hole in one's identity, which can be incredibly damaging to mental health. Identity is who a person is, the ego, all that we know we are. It is one of the only things we can grasp onto in this world. This tangent is very relevant, because of this chain of events: big tech companies look to make money, legal but unethical practices are used to keep eyes on screens. The longer eyes are on screen, the more money is made, but the more addicted users will become to the given apps, websites, or games. The more addicted users become to the product they consume, the less time they have to dedicate to something meaningful, something that will provide them with true purpose and pride. Without a purpose, the individual will have no passion, and may even lack confidence in their identity. This “hole” in one’s identity can lead to depressive symptoms, making somebody feel worthless because they essentially are. They are the real-life equivalent of the human batteries in The Matrix, except this time for everyone profiting from their time. This is big tech companies and the government, the latter being from economic stimulation.
	Unfortunately, it seems that these innovations are too far gone, and there is little that can be done to reverse the current damage we already are seeing. Truly the only solution now is to spread awareness of these issues and be mindful of them yourself. Make an effort to understand how the algorithms work to the best of your ability, so you have the best chance of making the conscious decision to listen to the speaker when spoken to. In other words, give yourself the ability to consent or not consent, decide when you want to scroll on Reddit or watch five minutes of TikTok. Do not let yourself fall into the natural habits of feeding into the dopamine addiction, because it will only waste the little time you have here. This seems dramatic, but it’s because it is dramatic. This is your life, and you should have the freedom to choose how you spend your time. The big tech companies are taking the choice away from the people, subsequently demeaning the right to pursue happiness that we were all born with. This is why this issue is so important to combat on an individual level, even more than a societal level. Woefully, many enjoy the comfort of addiction and therefore cannot be saved. Being aware of one’s actions and responses is the only way to move forward and live a happier life. Being introspective and taking a step back every once in a while will go a long way.
	The population is becoming more depressed, and in the public’s eye, there has been no obvious reason why. No public scrutiny on big tech companies has allowed for a lack of transparency on their part, essentially allowing them to solicit data from their users and make a profit off of their time. These business practices have proven to be unethical, but still legal, meaning the only action that can be taken is on an individual level. Awareness of one’s intakes is the key to improving many parts of life, and the internet is no exception. This awareness would allow for a new freedom from the addiction that phones have provided, which would let people lead more present lives. Furthermore, there would be more understanding as to what is a part of one’s identity, and what has been catalyzed by the big tech companies. To explain more simply, it would allow people to realize when they have been put into a thought bubble online, a place where all opinions are validated and radicalization is catalyzed. Then they would be able to take a step back to reevaluate what their true beliefs are. If this were to occur on a mass scale, society would look very different, as there would be less division amongst people who have varying beliefs. This is because hopefully one would understand that they have been manipulated to associate with a given group, only for the purpose of keeping them looking at the screen. Awareness is what would allow one to be more open minded and likely to listen, as it grants an opportunity to remove oneself from the thought bubble they have been placed in, subsequently allowing for one’s personal identity to grow stronger, which would then let the pursuit of happiness to continue. To see change, take a step to be more introspective and conscious, rather than letting big tech control your life. 

Works Cited

Twenge, Jean M., et al. “Increases in Depressive Symptoms, Suicide-Related Outcomes, and Suicide Rates Among U.S. Adolescents After 2010 and Links to Increased New Media Screen Time.” Clinical Psychological Science, vol. 6, no. 1, Jan. 2018, pp. 3–17, doi:10.1177/2167702617723376.
Yu, Bin, et al. “Distinct Associations of Computer/Mobile Devices Use and TV Watching with Depressive Symptoms in Adults: A Large Population Study in China.” Depression & Anxiety (1091-4269), vol. 36, no. 9, Sept. 2019, pp. 879–886. EBSCOhost, doi:10.1002/da.22932
Toronto, E. (2009). Time out of mind: Dissociation in the virtual world. Psychoanalytic Psychology, 26(2), 117–133. https://doi.org/10.1037/a0015485
Kang, Jin, and Liu Bingjie. "A Similarity Mindset Matters on Social Media: Using Algorithm-Generated Similarity Metrics to Foster Assimilation in Upward Social Comparison." Social Media + Society, vol. 5, no. 4, 2019. ProQuest, http://proxy.binghamton.edu/login?url=https://www-proquest-com.proxy.binghamton.edu/scholarly-journals/similarity-mindset-matters-on-social-media-using/docview/2331595786/se-2?accountid=14168, doi:http://dx.doi.org.proxy.binghamton.edu/10.1177/2056305119890884.
Graber, Mark A., and Abraham Graber. "Internet-Based Crowdsourcing and Research Ethics: The Case for IRB Review." Journal of Medical Ethics, vol. 39, no. 2, 2013, pp. 115. ProQuest,http://proxy.binghamton.edu/login?url=https://www-proquest-com.proxy.binghamton.edu/scholarly-journals/internet-based-crowdsourcing-research-ethics-case/docview/1781097829/se-2?accountid=14168, doi:http://dx.doi.org.proxy.binghamton.edu/10.1136/medethics-2012-100798.
Oulasvirta, A., Rattenbury, T., Ma, L. & Raita, E. (2012). Habits make smartphone use more pervasive. Personal and Ubiquitous Computing, 16(1), 105–114. 10.1007/s00779-011-0412-2
“2018 Report: Data-Backed Secrets of Winning Push Notification Campaigns.” CleverTap, CleverTap, 2018, info.clevertap.com/hubfs/Push_Notification_Report_2018.pdf?_hsmi=64135115&amp;_hsenc=p2ANqtz-_QBluBMA8qcjAYB0SuPhiNd3aHjTRF7KzKN2rBBfIpOtaQJpmw6VKRUjE6yMkJmlSwZbPC4vzIiXe3tkAgCto7P8DLiA. 
Pavlov, I. P. (1927). Conditioned Reflexes: An Investigation of the Physiological Activity of the Cerebral Cortex. Translated and Edited by G. V. Anrep. London: Oxford University Press. p. 142.
Ghaemi, S. N. “Digital Depression: a New Disease of the Millennium?” Wiley Online Library, 18 Jan. 2020, https://doi.org/10.1111/acps.13151
Hema Yoganarasimhan (2020) Search Personalization Using Machine Learning. Management Science 66(3):1045-1070. https://doi.org/10.1287/mnsc.2018.3255

Brian Matzelle
Writing 111
2 May 2021
Public Opinion Piece
	In my public opinion piece, I aim to dig deeper into an idea that was mentioned in my argumentative research essay, but not expanded upon. I look to focus on the controversy and dangers of data trading, with the privacy of online consumers being infringed upon. Specifically, consumers of applications and websites that follow ad-based revenue business models. The purpose of this is to bring awareness to the growing problem consumers are facing, as their privacy is slowly shrinking day by day without their knowledge of its occurrence. The current event has to do with Apple’s new privacy policy that looks to further protect its users. I attempt to win the reader over by using convincing data, as well as making portions relatable and applicable to the reader. This paper would fit well on any general technology website, with the target audience being those who are interested in either their own privacy, or the privacy of others.
	The first image I chose was of the notification that consumers of Apple products will begin to see after the newest update has been installed on their device. This was done to start the paper with something that may be applicable to the reader, so they are more likely to stay interested throughout the reading. Second, a pie chart of Facebook’s revenue distribution was shown, so the reader can see how all-encompassing the advertisements really are. This appeals to logos. The last image used was of Mark Zuckerberg, so the reader would have an adversary to visual, a central figure where the concern would be drawn to. This can help make the point seem more centralized, and therefore more coherent.
	It is crucial that the overarching business practices are realized, as privacy issues are only a byproduct of the growing problem at hand. It has become ubiquitous to use unethical practices that manipulate consumers, commonly causing addiction and depression that is not fully realized. For the sake of the consumer, it is imperative for a new light to be shined onto those in power—the big tech companies. The effects of their intrusions must be discussed, to help recover from the increasing numbers of depression that is now being recorded. Only after awareness is spread, only then, can a new plan be discussed on how to maintain happiness and mental health in society.

Apple’s New Privacy Policy, and How it Exposes Big Tech
Internet parasites, leeching off of your every move and looking to stay attached to you, the host, for as long as possible. This is what online trackers are—parasites that are meant to gather data from consumers so it can later be sold to advertisers for profit. It is not innocent in any way, and big tech companies like Facebook and Google understand this. That is why they have made an active effort to keep their users in the dark by being vague with their privacy policies, and not admitting the intrusiveness of their business practices. This of course leads to worry from those who understand the value personal information can have, especially now since almost all of someone’s sensitive information is stored in one place. Apple is a big tech company that has historically defended its users' data understanding the importance of privacy. Recently they have sparked controversy amongst the big tech companies, as they continue to act in character. Apple has just released iOS 14.5, a new update that most notably features the App Tracking Transparency (ATT) feature, which aims to put the power of privacy back into the users hands. The feature allows consumers to decide whether or not they will allow an app to track activity across applications, a practice that was common for many big companies to take part in.
 Pictured here is the notification that iPhone users will begin to receive after installing the newest update. The choice of privacy is now right in front of the user, rather than hidden deep in the settings, where it was previously. Apple’s move was controversial because of the consequences many other companies will be faced with, as much of their advertising revenue will be lost. To clarify again, applications will still be able to track users actions on their own platform regardless of how the notification is responded to. The difference now is that if the user answers “Ask App not to Track” on the prompt, companies will not be able to track data across apps for long periods of time, essentially gathering more data than is necessary. Data trading is a practice that should be scrutinized, given the sensitive nature of personal information that is now on almost every smartphone owned. Conceptually thinking, the more data that is in one place, the more valuable that place is. Now rather than having sensitive information (e.g. financial info, medical info) scattered in different files and papers, it is all on one smartphone. This is why data is said to be more profitable than oil in today’s time, which is something that Facebook, Google, and other leading tech companies are most aware of (Fauerbach). In fact, they are at the forefront of the data trading world, leading the pact when it comes to selling user’s personal data to advertisers. Given that “about 97.9 percent of Facebook's global revenue was generated from advertising,” it is unsurprising that they have spent money hoping to convince consumers to allow their data to be shared (Tankovska). Facebook has taken a large financial hit from this update, as “a report by Flurry Analytics states that up to 96 percent of all users are already disabling app tracking on iPhones in USA” (“Apple vs Facebook: 96 Percent Users Disabling App Tracking So Far, Claims Report”). Facebook bought full pages in newspapers to warn against the repercussions this update could have for small businesses, diverting from the reality of their own loss of income (Gartenberg).  Unfortunately, this market succeeds at the expense of the consumers, which is why Mark Zuckerberg and Facebook have made such an effort to keep the public oblivious to what is done with their data. This proves the true intention of big tech companies, which is to make the most money by any means necessary, without regard for the consumer. This same motif is what has led to negative effects society is witnessing now, through rising numbers of depression and other mental health concerns.
	The increasing numbers of depressive symptoms in society has been facilitated by a business culture that favors capital first before its consumers. This is clear, especially when the statistics are shown to connect the cause and effect. Big tech companies look to sell your data, as explained earlier. Being that this is their goal, they will do whatever is in their power to gain as much data as possible so they can make the most profit. Again, this was proven earlier by highlighting Facebook’s trackers that follow users across applications. It is concerning for privacy, yet it is done anyways so maximum profit is made. This same lack of morality carries over into the applications themselves, unfortunately it is not exclusive to only data tracking. Manipulative tactics have become ubiquitous amongst the top applications on mobile devices, it is impossible to compete without using them. Psychological methods such as classical conditioning and instant gratification are being experimented with on a massive scale, one that has caused much of the distress now seen in society. These methods abuse necessary biological functions, specifically affecting dopamine regularity, the chemical reward section of the brain. It is concerning, to say the least. Dopamine is a chemical that is released in the brain to reward someone for serving biological needs. For example, when you eat, dopamine is released. When you have sex, dopamine is released. When you exercise, dopamine is released. These are all necessities, unlike the use of social media. TikTok, Facebook, and Google do not care though, you will keep receiving bursts of dopamine everytime you scroll to see the next post, because that is what will keep your eyes on the screen. That is what will gather them more data from you, data that they will sell to any advertiser interested in paying for it. Incredibly adverse effects are now being seen, with depression rising because of the addiction that many people have, one that they have yet to realize. The youth feels entrapped, purposeless without any real hobbies or skills. 45% of American teens claim to be online “almost constantly,” most of them spending time on apps that only look to steal their time for a profit (Ghaemi 2). The data trading culture has made us human batteries for big tech companies, and the leaders understand this. That is why they have made every effort possible to shield the public from this truth. To reiterate, Facebook bought out full pages in newspapers just to divert the effects of Apple’s newest update away from themselves, looking to draw sympathy from readers claiming that small businesses will be hurt the most. This simply is not true, they are only looking to sustain the data trading market’s current state for as long as possible, because as soon as people wake up and see the manipulation that is occurring, regulation will ensue, whether it is from the people or from the government.
	Apple’s newest update shines light on a problem that is much more vast than many realize. Yes, privacy is at stake which is of utmost importance. However, there is a much more worrying set of values that is held by the most powerful companies in the world, a set of values that looks to benefit only the top of the hierarchy. Privacy issues are merely an effect of this set of values, just as increasing depression in our society is. It is understandable to take this update as a small victory, but it is imperative that the consumers recognize the unethical business practices underlying throughout every top application, on every app store, on every mobile device. Facebook will be hurt from this financially, but the company is sure to bounce back quickly, and with repercussions that are minor compared to the faults they have committed. That being said, Facebook is really interchangeable with any top tech company in the world. It is the only company used here because it is most relevant to the current event. Every other company will be affected financially, but new innovations for mining users data will still be found every day.
	It is time for the actions of big tech to be realized, the people have been manipulated and experimented on for too long. Children are growing up in a time that is foreign to their parents, dealing with concerning habits facilitated by unethical business practices, which then commonly trigger addiction and depression. Awareness of these tactics must be spread with the hopes that addiction could be realized as soon as possible, then providing some with the chance for an escape from unhappiness. Once reality is accepted, those who have been affected can look to take the first step forward, putting the power of choice back into the users hands. Once the decision to traverse social media is made consciously, a new door opens and self regulation can begin. Then, it is thought that the rising numbers of depression would subside, and society would have a chance to recover from the largest psychological experiment ever conducted. This is absolutely necessary, as right now many people do not even have a chance, as they struggle with an addiction they are unaware of.

Works Cited
“Apple vs Facebook: 96 Percent Users Disabling App Tracking So Far, Claims Report.” News18.Com: CNN-News18 Breaking News India, Latest News Headlines, Live News Updates, News18, 9 May 2021, www.news18.com/news/tech/apple-vs-facebook-96-percent-users-disabling-app-tracking-so-far-claims-report-3720917.html.
Fauerbach, Therese, and Name *. “Data Reigns in Today's Data Economy.” The Northridge Group, 28 Sept. 2020, www.northridgegroup.com/blog/more-valuable-than-oil-data-reigns-in-todays-data-economy/.
 Gartenberg, Chaim. “Why Apple's New Privacy Feature Is Such a Big Deal.” The Verge, The Verge, 27 Apr. 2021, www.theverge.com/2021/4/27/22405474/apple-app-tracking-transparency-ios-14-5-privacy-update-facebook-data.
 Ghaemi, S. N. “Digital Depression: a New Disease of the Millennium?” Wiley Online Library, 18 Jan. 2020, https://doi.org/10.1111/acps.13151
Tankovska, H. “Facebook Ad Revenue 2009-2018.” Statista, 5 Feb. 2021, www.statista.com/statistics/271258/facebooks-advertising-revenue-worldwide/#:~:text=In%202020%2C%20about%2097.9%20percent,increase%20in%20comparison%20to%20the.


Jr., Bill Murphy. “Here's the 1 Brutal Truth About Mark Zuckerberg Nobody Wants to Admit.” Inc.com, Inc., 3 Jan. 2019, www.inc.com/bill-murphy-jr/heres-1-brutal-truth-about-mark-zuckerberg-nobody-wants-to-admit.html.
Heilweil, Rebecca. “Why the New IOS Update Is Such a Big Deal.” Vox, Vox, 26 Apr. 2021, www.vox.com/recode/22393931/facebook-ios-14-5-app-tracking-transparency-iphone-privacy.
Dunn, Jeff. “The Tech Industry Is Dominated by 5 Big Companies - Here's How Each Makes Its Money.” Business Insider, Business Insider, 26 May 2017, www.businessinsider.com/how-google-apple-facebook-amazon-microsoft-make-money-chart-2017-5.
---END OF ETHICAL ACADEMIC PAPER---


---BEGINNING OF TECHNICAL RESEARCH PAPER---
Newton’s Method, and the Future of Optimization Algorithms 

Brian Matzelle
Department of Computer Science, Binghamton University
CS301 Ethical, Social, and Global Issues in Computing
April 23, 2022

Abstract
	Newton’s Method optimization algorithms continue to evolve as research progresses, yielding algorithms that compete with first-order algorithms, outperforming them in ways. Second-order algorithms have proven more robust than all first-order algorithms, requiring less data-labeling when programming, which eases data analysis processing. The more precise second-order methods yielded from research have the potential to greatly increase public safety, as machine learning continues to exist in new technologies. Common complaints from programmers in the past fail to hold up, as research has made Newton’s Method algorithms easier to implement, while maintaining reliability when used. As research innovates second-order optimization methods, the argument against utilizing new findings weakens. Newer algorithms created provide more precise results, one of the most important factors of an algorithm when public health remains a concern.

Newton’s Method, and the Future of Optimization Algorithms 
	Mistakenly thought of as the ideal optimization method for deep learning, gradient descent, a once fruitful method, no longer provides the best results. As research and technological advances continue, reevaluation of iterative optimization algorithms must occur to determine the most efficient method. Due to deep learning’s mathematical nature, computing solutions can overwhelm hardware and limit development for the researcher. For this reason, questions regarding memory usage and time complexity commonly appear on forums when comparing and choosing what method to use. Other concerns about accuracy and success rates require comparison as well. With the growing importance of optimization algorithms, Newton’s Method has proved impressive by out-performing the Gradient Descent Method, allowing for faster training in machine learning for new technology such as self-driving cars, which will improve the general quality of life but also save lives. With more algorithmic research and memory-related technological advancements, Newton’s Method will become the more efficient deep learning method. Researchers began making strides by introducing new techniques to counter common criticisms. Some examples would be the Saddle-Free Newton method (SFN),  Quasi-Newton methods, and other variations of Newton’s Method. As for now, gradient descent methods remain more popular, as their easy implementation and low-cost bypass current hardware restrictions. Recent discoveries mitigate or entirely avoid taxing aspects of second-order methods to make them more appealing. Variations of the Gradient Descent Method arose in recent years and stay popular, but the sprint seen with Newton-type Methods triumphs over current research progress of first-order methods.

Alternate Technology
	Generally accepted as the most practical method, gradient descent serves as the foundation for many machine learning applications because of its simple implementation. Used by large companies and independent programmers, the method achieved ubiquity amongst deep learning applications for its accessibility. Gradient descent iterates over a function to find a local minimum, by taking a step and analyzing the gradient. If the slope moves towards the local minimum, the algorithm will take a step in the same direction until it locates the local minimum. After many iterations, the algorithm optimizes the function, as local minima represent a set of optimal inputs for the application. Compared to other methods, gradient descent and its variations boast simplicity, categorized as first-order algorithms. First-order algorithms only analyze the gradient matrix, the first derivative of a scalar-value function.

Support
 A second-order method—such as Newton’s Method—must calculate and analyze the Hessian as well. The Hessian (a matrix of data about the second derivative of a scalar-value function) requires heavy computation, so gradient descent avoids it entirely, making up for less precision with cheaper, faster iterations. Gradient descent using first-order data makes it accessible for programmers since it takes less memory to compute solutions and train software. By taking momentum into account, the Hessian allows for faster convergence—the benefit of second-order algorithms. Hardware has advanced at a slower pace when compared to math theory, so gradient descent wins in popularity for now. Tesla, a car company known for its advanced software integration, uses gradient descent to train different software, ranging from learning the user’s preferences to allowing the car to drive itself. In the former example, the speed of innovation lacks importance, since innovating the user experience leads to preferential results. As for the latter example, the rate at which machines learn can affect the safety of the driver and those on the road. If a faster, more efficient algorithm existed, technical restrictions would lessen, then allowing companies to focus on addressing ethical questions (e.g. addressing and preparing for possible accident cases) as they arise. Gradient descent works, but its potential lacks when compared to Newton’s Method and its variations. Recent strides in research and development have made second-order algorithms faster, more efficient, and increasingly accessible for programmers.
	Many programmers in the world of deep learning and neural falsely believe Newton’s method remains underdeveloped and underutilized. A lack of exposure to recent mathematical and technological advancements explains common misconceptions surrounding this method. A criticism of the method includes the failure to avoid saddle points when optimizing a program. In the saddle point case, variables found by the algorithm represent an incorrect minimum. The failure occurs because Newton’s Method iterates over the given function until the gradient equals 0, a characteristic of both minima and saddle points. Specifically, non-convex functions struggle with this problem. For some time the criticism was fair, but recently strides in mathematics have counteracted this issue. Researchers have created modified versions of the algorithm categorized as Quasi-Newton Methods that tackle common issues of the unmodified method. Research by Truong, an Associate Professor at the University of Oslo, Norway (2021), introduced a new modification named the New Q-Newton’s Method that has proved more effective than any other variation so far, as it eliminates the saddle point problem. New Q-Newton’s Method differs from other variations greatly, requiring orthogonal diagonalization of real symmetric matrices when analyzing the first-order data. This process sets the method apart from other Newton’s Method variations, because having an orthogonal decomposition of the data set allows for more precise estimates of the Hessian, without computing the arduous matrix (the Hessian). In short, orthogonal diagonalization allows for more precise variable predictions while maintaining optimal iteration speeds with smaller sets. With this discovery, non-convex data sets no longer cause saddle point errors for the second-order algorithm, eliminating the largest criticism on the small list of cons. Efforts by researchers alike created this trend of improvement, eliminating the poor traits of Newton’s Method study by study. The downside of orthogonal diagonalization affects the algorithm's implementation potential, as the expensive process greatly slows each iteration when large amounts of data are analyzed. For this reason, implementing the findings of New Q-Newton’s Method into another second-order algorithm that handles large data sets would prove most fruitful, bringing out the best qualities from each algorithm.
	With the amount of research on Newton’s Method, it will not take long for the algorithm to outperform first-order methods. Researchers have taken different approaches when improving the algorithm. A successful angle was taken by Jinghui Chen, Assistant Professor in the College of Information Sciences and Technology at Penn State University (2017). Chen and his team broke down faster algorithms and noted what they did efficiently, then applied that knowledge to a variation of Newton’s Method. With this goal in mind, Chen analyzed stochastic gradient descent, specifically how it estimates the trajectory of the data provided. In other words, the team observed how the algorithm decides whether the iteration moved towards the minimum, or away from it. Stochastic gradient descent analyzes a small portion of the data which allows for quick estimates, a concept Chen applied to this new variation. Applying this alongside the recursive definition in Figure 3 (Chen, 2017) reduces space complexity from O(d2)to O(d), a counter to the common complaint of expensive iterations. Chen presents the Fast Newton Thresholding Pursuit (FNHTP) algorithm, a notably faster Newton’s Method. As previously mentioned, the New Q-Newton algorithm has already bypassed FNHTP, continuing the research towards stochastic predictions that started with this method. In a matter of four years, mathematical innovations have made Newton’s Method more applicable by avoiding saddle points, and more efficient by introducing recursion and stochastic techniques. The gap in performance between first and second-order methods has decreased substantially, because of the combined small steps of researchers. 
	Researchers such as Chien-Chih Wang from the Rakuten Institute of Technology, and Kent Loong Tan from National Taiwan University (2020) continue exploring different ways of applying Newton’s Method to Python code via MATLAB. Through research they have created building blocks to follow, that way programmers can choose the preferred variation to implement.  Understanding the negative connotation that Newton’s Method holds, the team repeatedly mentions the simplicity of the implementation, given that their algorithm only requires a few hundred lines of code. Applying the method to programs has drawn much concern from programmers on forums, but these efforts weaken the argument as research continues. As mentioned by Tuyen Truong—a researcher previously mentioned—on Stack Exchange, the high cost of implementation with no guarantee of convergence makes developers think twice before committing to Newton’s Method (user292463, 2020). Again, Wang and Tan’s building blocks for implementation mitigate this complaint, as their library acts as a baseline Quasi-Newton’s Method and Newton’s Method support. Boasting even more utility, the library contains support for Convolutional Neural Networks (CNN), a ubiquitous deep learning network type. The high ceiling of Newton’s Method allows for much room to improve. In time, increasing support promises faster convergence and fewer failures regarding non-convex data sets. Programmers on the cutting edge of machine learning have begun to see the importance of research toward second-order optimization algorithms. Second-order algorithms converge to minima with fewer iterations, which has sparked questions amongst programmers when choosing the most efficient optimization algorithm.
	Deep learning rose to the forefront of interest in recent years, serving as the foundation for many novel technologies in development. Notably, driving cars and artificial intelligence bring new challenges to the table, different from other technologies that utilize neural networks. Yann LeCun, the AI Chief of Meta, vouches for the use of self-supervised learning when creating neural networks (Strickland, 2022, para. 2). Self-supervised learning refers to a way of teaching a machine learning program that bypasses the need for human guidance. In turn, programmers would not spend unnecessary time guiding the machine, which costs the company more financially in the end. In paragraph 6, Strickland notes that according to LeCun, self-supervised learning “makes the system more robust, and more able to handle inputs that are different from the labeled training samples.” Wang, a previously mentioned researcher (2020), uses similar reasoning when arguing in favor of second-order methods. Wang claimed self-supervised learning allows for larger data sets and less specific data typing, which creates a more accurate program. LeCun explains this with artificial intelligence (AI), explaining that the program will return more accurate results when trained with self-supervised learning. The futuristic training method needs more research and development but would benefit from implementation via second-order algorithms.
Using a variation of Newton’s Method with LeCun’s training suggestion promises fast convergence alongside robust data analysis, traits nonexistent with gradient descent methods. This would greatly improve personal assistants, e.g., Apple’s Siri, Amazon’s Alexa, Microsoft’s Cortana, and similar AI. With more accurate neural networks, assistants would communicate more effectively, better understand the user, and possibly personalize responses to the user. As a result, humans would access information faster, have personalized assistants, and lead more comfortable lives. More important than convenience though, health and safety remain the most ethically important aspect of society to improve.
	Following the continued struggle to understand a recent climb of depression and anxiety amongst youth, psychiatrists now utilize machine learning to find answers. Dr. Lebrun-Harris, a Senior Social Scientist at the Health Resources and Services Administration, studied rising depression and anxiety rates—increasing 27% and 29%, respectively—from 2016 to 2020 (2022). An urgency exists now due to the life-threatening implications of depression and anxiety. Dr. Ellen Lee, an assistant professor of psychiatry at the University of California San Diego, relays the importance of using cutting-edge technology to address the problem quickly growing (Lindsey, 2021). Lee explains the many different data types requiring analysis, packaged in large data sets that would create a patient profile. Given the robust nature of Newton’s Method, the second-order algorithm could analyze patient habits with less programmer labeling of data necessary. Less labeling would facilitate larger variable sets, allowing for more accurate patient profiling when compared to results from gradient descent methods. An ethical obligation presents itself here since the topic relates to the health and safety of the subject. A psychiatrist ought to use the most precise and accurate data available when treating patients. 
Even if Newton’s Method takes longer to research and implement, it presents the most accurate results when analyzing data. When prescribing patients with medication to fight life-threatening illnesses, small data imperfections can make an unfortunate difference. Harald Scheidl (2021), Senior Computer Vision Engineer at Sportradar, created a library to visually represent the difference in accuracy between stochastic gradient descent and AdaHessian (a variation of Newton’s Method). Figure 4 (Scheidl, 2021) shows the paths taken by each algorithm as well. Identifying correct, quantifiable causes of increased depression and anxiety rates would save lives, while decreasing debate of analysis methods within the psychiatric community. Greater confidence amongst psychiatrists would help to avoid misdiagnoses, an alarming issue within the profession. In 2011, a study was conducted measuring misdiagnosis rates between 7 primary care clinics. According to the study, “misdiagnosis rates reached 65.9% for major depressive disorder, 92.7% for bipolar disorder, 85.8% for panic disorder, 71.0% for generalized anxiety disorder, and 97.8% for social anxiety disorder” (Vermani, 2011). A second-order algorithm serves as the direct solution, theoretically creating an error-free way to analyze patients’ conditions. In addition, the social stigma that surrounds mental health treatment would likely deteriorate, since mathematically defining the causes of illness prove its existence. The accuracy and reliability of Newton’s Method algorithms make the choice easy for the medical field, and most importantly for the patient. Generally, fields requiring considerations towards health and safety share similar concerns, making second-order algorithms more necessary than many realize.
	Winning over public opinion, self-driving cars transformed from science fiction to reality in a short time. Low-level self-driving cars traverse the streets daily, sporting consumers eager to show off the expensive purchase. Health and safety remain at the forefront of concern for developers, as the lives of passengers lie in their code. At this time, self-driving cars rely on drivers to account for mistakes the vehicle may make, forcing them to stay attentive to the road in different ways. Tesla, for example, placed a driver monitoring camera in the car as an extra precaution, making sure the driver stays attentive. Public opinion praised this addition, reinforcing the value of safety, no matter how new the technology (Barry, 2021, para. 2). Second-order methods continue to dominate this debate, but for different reasons when it comes to self-driving cars.
	Understanding how self-driving cars read, analyze, and drive on the road is crucial when analyzing different optimization algorithms to use. David Silver (2017), a Senior Software Engineer at Cruise, explains the process in 5 steps during his TED Talk. Steps 1 and 2 work simultaneously to collect data using Computer Vision (CV) and Sensor Fusion (SF). Here, deep neural networks are communicated to and trained, letting the car “see” the road. Moving on to the next step, localization refers to the car deciding its location in relation to the surrounding environment. Following that comes path planning, where the car creates a line on the road to drive along. In the final step, the software dictates the next move of the car and controls the hardware.
	Modulating the self-driving process will help articulate exactly where machine learning concerns originate. Beginning with step 1, training self-driving cars to “see” and understand the world around them with first-order methods requires tedious effort from the developers. Xu (2018) points out this flaw when after various Newton-type algorithms, reiterating that second-order methods’ performance stays consistent regardless of hyper-parameter choices. Consistency results in fewer errors and more universal coverage of data when machine learning. If applied to self-driving technology, second-order integration would increase the adaptability of cars, preventing errors or confusion when unpredictable objects appear. This greatly increases safety for pedestrians and other drivers, because programmers could never account for every unexpected event. As stated previously, the ethical obligation to create the most robust system applies here to protect consumers. Programmers must choose the system least likely to contain bugs and inaccuracies when the risks involve health and safety. Implementation of Newton-type methods would also decrease the amount of fine-tuning errors that may occur when coding these deep neural networks. Once implemented, programmers spend less time babysitting the program, making changes, and creating data types. Instead, they would have more time to answer difficult ethical questions and improve other aspects of the program. Consumers would benefit most, living safer, more comfortable lives. Positive public sentiment would also increase funding and support for self-driving cars, therefore increasing safety even more as research progresses.
Conclusion
	Self-driving cars serve as one of many examples when discussing machine learning’s importance in future technology. Optimization algorithms have started to take over the world of software development, allowing for safer, smarter, and overall better tailored programs for consumers. With the inclusion of machine learning, discussions of performance must align with safety concerns as well. As computers take control of machines that risk of life-threatening accidents, programmers must continue to use the most reliable methods. Research toward Newton’s Method algorithms has aimed to provide a more robust machine learning method, which would solve many precision concerns that developers may have.
	Utilization of the most robust and precise optimization algorithm would dismiss many safety concerns the public has, lessening the chance of unexpected failures to occur. When safety comes into question, computer professionals must choose the safest method, rather than the most convenient method. Newton’s Method has proved more robust than Gradient Descent methods, while also promising more precise results. With more development of easier second-order implementations, computer professionals lack the ethical footing to argue in favor of more dangerous first-order methods. Slothful programming might make a small difference in user interface development, but a life-threatening difference in medicine or transportation, which makes the choice of algorithm obvious.

References/Works Cited
Aksakal. (2017, September 6). Why is Newton's Method not widely used in machine learning. [Image]. Stack Exchange. https://stats.stackexchange.com/q/301728 
Barry, K. (2021, May 27). Cr engineers show a Tesla will drive with no one in the driver's seat. Consumer Reports. https://www.consumerreports.org/autonomous-driving/cr-engineers-show-tesla-will-drive-with-no-one-in-drivers-seat/ 
Chen, J., & Gu, Q. (2017). Fast Newton hard thresholding pursuit for sparsity constrained nonconvex optimization. Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, (pp. 757–766). 10.1145/3097983.3098165
Crypto1. (2020, October 5). How does the gradient descent algorithm work in machine learning? Analytics Vidhya. https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/#:~:text=Gradient%20descent%20is%20an%20iterative,function%20at%20the%20current%20point. 
Lebrun-Harris, L. A., Ghandour, R. M., Kogan, M. D., & Warren, M. D. (2022). Five-year trends in US Children’s health and well-being, 2016-2020. JAMA Pediatrics. https://doi.org/10.1001/jamapediatrics.2022.0056 
Lindsey, H. (2021, September 30). AI's push to understand psychiatry research has the potential to tackle mental illness. Business Insider. https://www.businessinsider.com/how-ai-is-advancing-psychiatry-medicine-to-tackle-mental-illness-2021-9#:~:text=AI%20enables%20researchers%20to%20better,clinicians%20to%20adopt%20the%20technology. 
Vermani, M., Marcus, M., & Katzman, M. A. (2011). Rates of detection of mood and anxiety disorders in primary care: A descriptive, cross-sectional study. The Primary Care Companion for CNS Disorders, 13(2), PCC.10m01013. https://doi.org/10.4088/PCC.10m01013
Scheidl, H. (2021, August 13). Adahessian: A second-order optimizer for deep learning. [Image]. Medium. https://towardsdatascience.com/adahessian-a-second-order-optimizer-for-deep-learning-2fc76b29bcbb 
Silver, D. (2017, November 28). How self-driving cars work | David Silver | TEDxWilmingtonSalon. [Video]. Youtube. https://www.youtube.com/watch?v=Ly92UcnoEMY&ab_channel=TEDxTalks
Strickland, E. (2022, February 23). Yann Lecun: AI doesn't need our supervision. IEEE Spectrum. https://spectrum.ieee.org/yann-lecun-ai 
Truong, T. T., To, T. D., Nguyen, T. H., Nguyen, T. H., Nguyen, H. P., & Helmy, M. (2021). A fast and simple modification of Newton’s method helping to avoid saddle points. ArXiv:2006.01512 [Cs, Math, Stat]. https://doi.org/10.48550/arXiv.2006.0151
user292463. (2020, August 3). Why is Newton's Method not widely used in machine learning. Stack Exchange. https://stats.stackexchange.com/q/479199
Visually Explained. (2021, January 19). Visually explained: Newton's Method in optimization [Video]. Youtube. https://www.youtube.com/watch?v=W7S94pq5Xuo&t=164s&ab_channel=VisuallyExplained
Wang, C.-C., Tan, K. L., & Lin, C.-J. (2020). Newton Methods for Convolutional Neural Networks. ACM Transactions on Intelligent Systems and Technology, 11(2), 19:1-19:30. 10.1145/336827
Xu, P., Roosta-Khorasani, F., & Mahoney, M. W. (2018, February 16). Second-order optimization for Non-Convex Machine Learning: An empirical study. arXiv.org. https://arxiv.org/abs/1708.07827 
---END OF TECHNICAL RESEARCH PAPER---